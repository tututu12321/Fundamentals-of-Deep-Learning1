import torch
import torch.nn as nn
import torch.optim as optim

# シンプルなRNNモデルを定義
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        
        # RNNの重み
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        # 出力層
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden):
        out, hidden = self.rnn(x, hidden)  # RNNの順伝搬
        out = self.fc(out[:, -1, :])       # 最後の時点の出力を使って分類
        return out, hidden

    def init_hidden(self, batch_size):
        # 隠れ層を初期化（ゼロに設定）
        return torch.zeros(1, batch_size, self.hidden_size)

# モデルのパラメータ
input_size = 10   # 入力の次元数（特徴量の数）
hidden_size = 20  # 隠れ層のユニット数
output_size = 1   # 出力の次元数（回帰タスクの場合は1）

# モデル、損失関数、オプティマイザの設定
model = SimpleRNN(input_size, hidden_size, output_size)
criterion = nn.MSELoss()  # 損失関数（回帰の場合の平均二乗誤差）
optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adamオプティマイザ

# ダミーデータ（バッチサイズ=5、シーケンス長=3、入力サイズ=10）
batch_size = 5
sequence_length = 3
x_train = torch.randn(batch_size, sequence_length, input_size)  # 入力データ
y_train = torch.randn(batch_size, output_size)                  # 出力データ

# 訓練ループ
num_epochs = 100

for epoch in range(num_epochs):
    model.train()  # 訓練モードに設定

    # 隠れ層を毎回リセット
    hidden = model.init_hidden(batch_size)

    # 順伝搬
    outputs, hidden = model(x_train, hidden)
    loss = criterion(outputs, y_train)

    # 勾配のリセット
    optimizer.zero_grad()

    # 逆伝搬
    loss.backward()

    # パラメータの更新
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# テスト
model.eval()  # 推論モードに設定
with torch.no_grad():
    test_input = torch.randn(batch_size, sequence_length, input_size)
    test_hidden = model.init_hidden(batch_size)
    test_output, _ = model(test_input, test_hidden)
    print(f'Test Output: {test_output}')
