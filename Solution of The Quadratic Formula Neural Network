import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# 解の公式を使って解を計算
def solve_quadratic(a, b, c):
    discriminant = b**2 - 4*a*c
    if discriminant < 0:
        return None  # 実数解がない場合
    x1 = (-b + np.sqrt(discriminant)) / (2*a)
    x2 = (-b - np.sqrt(discriminant)) / (2*a)
    return x1, x2

# 学習データを生成
def generate_data(num_samples=1000):
    X = []
    y = []
    for _ in range(num_samples):
        a = np.random.uniform(1, 10)  # aの値をランダムに設定
        b = np.random.uniform(-10, 10)  # bの値をランダムに設定
        c = np.random.uniform(-10, 10)  # cの値をランダムに設定
        roots = solve_quadratic(a, b, c)
        if roots is not None:
            X.append([a, b, c])
            y.append(roots)
    return np.array(X), np.array(y)

# 学習用データを生成
X, y = generate_data(num_samples=5000)

# データの正規化（StandardScalerを使用）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# データをPyTorchのテンソルに変換
X_train = torch.tensor(X_scaled, dtype=torch.float32)
y_train = torch.tensor(y, dtype=torch.float32)

# ニューラルネットワークの定義
class QuadraticSolverNet(nn.Module):
    def __init__(self):
        super(QuadraticSolverNet, self).__init__()
        self.fc1 = nn.Linear(3, 256)  # 入力は係数a, b, cの3つ
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 2)  # 出力は2つの解 (x1, x2)
        self.dropout = nn.Dropout(0.3)  # 過学習を防ぐためにドロップアウトを追加
        self.batch_norm1 = nn.BatchNorm1d(256)
        self.batch_norm2 = nn.BatchNorm1d(128)
        self.batch_norm3 = nn.BatchNorm1d(64)

    def forward(self, x):
        x = torch.relu(self.batch_norm1(self.fc1(x)))
        x = self.dropout(x)
        x = torch.relu(self.batch_norm2(self.fc2(x)))
        x = torch.relu(self.batch_norm3(self.fc3(x)))
        x = self.fc4(x)
        return x

# モデルの初期化
model = QuadraticSolverNet()
criterion = nn.MSELoss()  # 平均二乗誤差
optimizer = optim.Adam(model.parameters(), lr=0.001)

# モデルのトレーニング
def train_model(model, X_train, y_train, epochs=2000):
    losses = []
    for epoch in range(epochs):
        model.train()

        # 順伝播
        outputs = model(X_train)
        loss = criterion(outputs, y_train)

        # 逆伝播と最適化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        losses.append(loss.item())
        if (epoch+1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')
    
    return losses

# トレーニング実行
losses = train_model(model, X_train, y_train)

# トレーニングの損失をプロット
plt.plot(losses)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

# テスト時にバッチ正規化とドロップアウトを無効にするために評価モードにする
model.eval()  # 評価モードに切り替え

# テストデータで予測
with torch.no_grad():  # 勾配計算を無効にする
    test_data = np.array([[1.0, -3.0, 2.0]])  # テスト用の a=1, b=-3, c=2
    test_data_scaled = scaler.transform(test_data)  # データを正規化
    test_tensor = torch.tensor(test_data_scaled, dtype=torch.float32)  # テンソルに変換
    predicted_roots = model(test_tensor)  # 予測

# 予測された解を表示
print(f'Predicted roots: {predicted_roots.detach().numpy()}')

# 正解: 解の公式で求めた値
true_roots = solve_quadratic(1, -3, 2)
print(f'True roots: {true_roots}')
