import numpy as np

# アフィンレイヤのクラス
# Affine Layer class
class AffineLayer:
    def __init__(self, input_size, output_size):
        # 重みとバイアスの初期化
        # Initialize weights and biases
        self.weights = np.random.randn(input_size, output_size)
        self.biases = np.random.randn(1, output_size)

        # 順伝播時に使用する入力値の保存用
        # Placeholder to store input during forward propagation
        self.x = None

    # 順伝播
    # Forward propagation
    def forward(self, x):
        # 入力を保存
        # Store the input
        self.x = x
        
        # アフィン変換 (xW + b)
        # Perform affine transformation (xW + b)
        return np.dot(x, self.weights) + self.biases

    # 誤差逆伝播
    # Backward propagation
    def backward(self, dout):
        # 入力x、重み、バイアスに対する勾配を計算
        # Compute gradients with respect to input x, weights, and biases
        dx = np.dot(dout, self.weights.T)
        dW = np.dot(self.x.T, dout)
        db = np.sum(dout, axis=0, keepdims=True)

        # 重みとバイアスの更新に使用するための勾配を返す
        # Return gradients to update weights and biases
        return dx, dW, db

# サンプル入力
# Sample input
X = np.array([[0.5, 1.0], [0.2, 0.8]])

# アフィンレイヤのインスタンスを作成
# Create an instance of the affine layer
affine_layer = AffineLayer(input_size=2, output_size=3)

# 順伝播
# Forward propagation
out = affine_layer.forward(X)
print("順伝播の出力: \n", out)

# 逆伝播のためのダミー勾配 (例として ones を使用)
# Dummy gradient for backpropagation (using ones as an example)
dout = np.ones((2, 3))

# 逆伝播
# Backward propagation
dx, dW, db = affine_layer.backward(dout)

# 結果を表示
# Display results
print("逆伝播の入力に対する勾配 dx: \n", dx)
print("重みに対する勾配 dW: \n", dW)
print("バイアスに対する勾配 db: \n", db)
